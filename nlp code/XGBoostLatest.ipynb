{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd drive/MyDrive/colab_final_project/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK40cNH28KjX",
        "outputId": "df3e452d-f043-405a-a7a8-ec10e5a1c3ec"
      },
      "id": "QK40cNH28KjX",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/colab_final_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.7\n",
        "!pip install numpy==1.23.3\n",
        "!pip install langdetect==1.0.9\n",
        "!pip install emoji\n",
        "!pip install scikeras\n",
        "!pip install torch==2.0.1\n",
        "!pip install scikit-learn\n",
        "!pip install gensim\n",
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_fh0JbyP8E41",
        "outputId": "f59c8a7d-be55-44f4-a79d-3aea831f10f2"
      },
      "id": "_fh0JbyP8E41",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.7\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.7) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.7) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.7) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.7) (4.66.1)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed nltk-3.7\n",
            "Collecting numpy==1.23.3\n",
            "  Downloading numpy-1.23.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "Successfully installed numpy-1.23.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect==1.0.9\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect==1.0.9) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=725c6b9bc37325acef0f5ee43be5084a4b275dddf0cbe34d6b9791520210b572\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.8.0\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.11.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.23.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.11.0\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (1.7.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.23.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "73798475",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73798475",
        "outputId": "f9532dfc-0189-4ddd-9118-f01e765bec40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "import time\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk.classify.util as util\n",
        "import itertools\n",
        "import pickle\n",
        "import csv\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import KeyedVectors\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import os.path\n",
        "from statistics import mode\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9a692413",
      "metadata": {
        "id": "9a692413"
      },
      "outputs": [],
      "source": [
        "emoji_dict = {}\n",
        "with open('emoji.txt', 'r', encoding='latin-1') as emoji_file:\n",
        "    for line in emoji_file:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            emoji, value = line.split('\\t')\n",
        "            emoji_dict[emoji] = int(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ee0f3a9e",
      "metadata": {
        "id": "ee0f3a9e"
      },
      "outputs": [],
      "source": [
        "def replace_emojis(text, emoji_dict):\n",
        "    for emoji, value in emoji_dict.items():\n",
        "        if value == 1:\n",
        "            text = re.sub(re.escape(emoji), 'happy', text)\n",
        "        elif value == -1:\n",
        "            text = re.sub(re.escape(emoji), 'sad', text)\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    text = ' '.join(filtered_words)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9ac4c3d7",
      "metadata": {
        "id": "9ac4c3d7"
      },
      "outputs": [],
      "source": [
        "hatefulCorpus = []\n",
        "nHatefulCorpus = []\n",
        "neutralCorpus = []\n",
        "labels = []\n",
        "hatefulCorpus_size = 600\n",
        "\n",
        "with open(\"merged.csv\", \"r\", encoding=\"utf-8\") as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "        if not all(value == \"\" for value in row.values()):\n",
        "            text = row[\"Comment Text\"]\n",
        "            if text == \"\":\n",
        "                continue\n",
        "            label = row[\"Label\"]\n",
        "            if label == \"\":\n",
        "                continue\n",
        "            newText = text.strip()\n",
        "            newText = replace_emojis(newText, emoji_dict)\n",
        "            newText = preprocess_text(newText)\n",
        "\n",
        "            if label == 'Neutral':\n",
        "                neutralCorpus.append(newText)\n",
        "                labels.append(label)\n",
        "            elif label == 'Hateful':\n",
        "                if len(hatefulCorpus) >= hatefulCorpus_size:\n",
        "                    continue  # Skip appending to hatefulCorpus\n",
        "                hatefulCorpus.append(newText)\n",
        "                labels.append(label)\n",
        "            elif label == 'Non Hateful':\n",
        "                nHatefulCorpus.append(newText)\n",
        "                labels.append(label)\n",
        "                labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nHatefulCorpus=nHatefulCorpus+nHatefulCorpus\n",
        "print(\"neutral = \"+str(len(neutralCorpus))+\" nhateful = \"+str(len(nHatefulCorpus))+\" hateful = \"+str(len(hatefulCorpus)))"
      ],
      "metadata": {
        "id": "EPCPu2JL9krP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2359e74b-087d-48bd-c199-a6e3d2f45f42"
      },
      "id": "EPCPu2JL9krP",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral = 603 nhateful = 598 hateful = 600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0cc9fac8",
      "metadata": {
        "id": "0cc9fac8",
        "outputId": "c86c7317-2398-4b42-9d04-d4af69c93d2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6149584487534626\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.55      0.56       126\n",
            "           1       0.67      0.84      0.75       105\n",
            "           2       0.59      0.50      0.54       130\n",
            "\n",
            "    accuracy                           0.61       361\n",
            "   macro avg       0.61      0.63      0.62       361\n",
            "weighted avg       0.61      0.61      0.61       361\n",
            "\n",
            "Training Time: 13.804874658584595 seconds\n"
          ]
        }
      ],
      "source": [
        "data = hatefulCorpus + nHatefulCorpus + neutralCorpus\n",
        "labels = [0] * len(hatefulCorpus) + [1] * len(nHatefulCorpus) + [2] * len(neutralCorpus)\n",
        "vectorizer = TfidfVectorizer()\n",
        "features = vectorizer.fit_transform(data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: XGBoost Classifier Training\n",
        "start_time = time.time()\n",
        "xgb_classifier = xgb.XGBClassifier()\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Step 4: Evaluation\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Training Time:\", training_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70aa02aa",
      "metadata": {
        "id": "70aa02aa",
        "outputId": "ca340830-7058-40f6-f726-722f010de2e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8dc1bd9f",
      "metadata": {
        "id": "8dc1bd9f",
        "outputId": "693f13d7-b55c-4075-a0aa-90317b541942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Hateful comments: 573\n",
            "Number of Non-Hateful comments: 619\n",
            "Number of Neutral comments: 609\n",
            "1801\n",
            "1801\n",
            "**************************************\n",
            "573\n",
            "619\n",
            "609\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "save_directory = 'savedModels'\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "filename = os.path.join(save_directory, 'xgb_model.pkl')\n",
        "filename2 = os.path.join(save_directory, 'xgb_tfidf.pkl')\n",
        "pickle.dump(vectorizer, open(filename2, 'wb'))\n",
        "pickle.dump(xgb_classifier, open(filename, 'wb'))\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "#new_text = [\"Send all the criminal immigrants out of Europe...NOW....\"]\n",
        "loaded_vect = pickle.load(open(filename2, 'rb'))\n",
        "new_features = loaded_vect.transform(data)\n",
        "prediction = loaded_model.predict(new_features)\n",
        "\n",
        "#count of zeros, ones, and twos\n",
        "count_zeros = 0\n",
        "count_ones = 0\n",
        "count_twos = 0\n",
        "\n",
        "\n",
        "for pred in prediction.flat:\n",
        "    if pred == 0:\n",
        "        count_zeros = count_zeros + 1\n",
        "    elif pred == 1:\n",
        "        count_ones = count_ones + 1\n",
        "    elif pred == 2:\n",
        "        count_twos = count_twos + 1\n",
        "print(\"Number of Hateful comments: %d\" % count_zeros)\n",
        "print(\"Number of Non-Hateful comments: %d\" % count_ones)\n",
        "print(\"Number of Neutral comments: %d\" % count_twos)\n",
        "\n",
        "print(prediction.size)\n",
        "print(count_zeros + count_ones + count_twos)\n",
        "\n",
        "print(\"**************************************\")\n",
        "class_labels = ['Hateful', 'Non-Hateful', 'Neutral']\n",
        "prediction_summary = {label: 0 for label in class_labels}\n",
        "\n",
        "values, counts = np.unique(prediction, return_counts=True)\n",
        "\n",
        "for val, cnt in np.nditer([values,counts]):\n",
        "    label = class_labels[val]\n",
        "    prediction_summary[label] = cnt\n",
        "\n",
        "print(prediction_summary['Hateful'])\n",
        "print(prediction_summary['Non-Hateful'])\n",
        "print(prediction_summary['Neutral'])\n",
        "\n",
        "#label_mapping = {0: 'hateful', 1: 'non-hateful', 2: 'neutral'}\n",
        "#predicted_label = label_mapping[prediction[0]]\n",
        "#print(\"Predicted label:\", predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b1eec918",
      "metadata": {
        "id": "b1eec918"
      },
      "outputs": [],
      "source": [
        "#word_embedding_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "arr = np.asarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3296dd63",
      "metadata": {
        "id": "3296dd63"
      },
      "outputs": [],
      "source": [
        "#Check later\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "corpus = neutralCorpus + hatefulCorpus + nHatefulCorpus\n",
        "labels = ['Neutral'] * len(neutralCorpus) + ['Hateful'] * len(hatefulCorpus) + ['Non Hateful'] * len(nHatefulCorpus)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Extract features using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 3: Define the XGBoost classifier\n",
        "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(label_encoder.classes_))\n",
        "\n",
        "# Step 4: Train the classifier\n",
        "xgb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = xgb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Step 6: Decode the predicted labels\n",
        "predicted_labels = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "# Step 7: Evaluate the accuracy\n",
        "accuracy = accuracy_score(label_encoder.inverse_transform(y_test), predicted_labels)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4133839e",
      "metadata": {
        "id": "4133839e"
      },
      "outputs": [],
      "source": [
        "# Create labels\n",
        "hateful_labels = np.ones(len(hatefulCorpus))\n",
        "nHateful_labels = np.zeros(len(nHatefulCorpus))\n",
        "neutral_labels = np.full(len(neutralCorpus), 2)\n",
        "\n",
        "# Combine data and labels\n",
        "data = nHatefulCorpus + hatefulCorpus + neutralCorpus\n",
        "labels = np.concatenate([hateful_labels, nHateful_labels, neutral_labels])\n",
        "\n",
        "# Convert text to word embeddings\n",
        "data_embeddings = []\n",
        "for text in data:\n",
        "    words = word_tokenize(text)\n",
        "    embeddings = []\n",
        "    for word in words:\n",
        "        if word in word_embedding_model:\n",
        "            embeddings.append(word_embedding_model[word])\n",
        "    if embeddings:\n",
        "        text_embedding = np.mean(embeddings, axis=0)\n",
        "        data_embeddings.append(text_embedding)\n",
        "\n",
        "data_embeddings = np.array(data_embeddings)\n",
        "#data_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12437773",
      "metadata": {
        "id": "12437773"
      },
      "outputs": [],
      "source": [
        "labels = labels[:data_embeddings.shape[0]]\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "params = {\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': 3,\n",
        "}\n",
        "\n",
        "xgb_classifier = xgb.XGBClassifier(**params)\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a25bdb",
      "metadata": {
        "id": "47a25bdb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "a = np.arange(15).reshape(3,5)\n",
        "print(a)\n",
        "print(a.ndim)\n",
        "print(a.dtype.name)\n",
        "print(a.itemsize)\n",
        "print(a.size)\n",
        "print(type(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987b0842",
      "metadata": {
        "id": "987b0842"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}