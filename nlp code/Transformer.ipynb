{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f93e9a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\babyj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\babyj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\babyj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.classify.util as util\n",
    "import itertools\n",
    "import pickle\n",
    "from nltk.probability import FreqDist\n",
    "import os.path\n",
    "from statistics import mode\n",
    "from nltk.classify import ClassifierI\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21494d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {}\n",
    "with open('emoji.txt', 'r', encoding='latin-1') as emoji_file:\n",
    "    for line in emoji_file:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            emoji, value = line.split('\\t')\n",
    "            emoji_dict[emoji] = int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91106086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis(text, emoji_dict):\n",
    "    for emoji, value in emoji_dict.items():\n",
    "        if value == 1:\n",
    "            text = re.sub(re.escape(emoji), 'happy', text)\n",
    "        elif value == -1:\n",
    "            text = re.sub(re.escape(emoji), 'sad', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1ac01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vincent\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenize the words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged = pos_tag(words)\n",
    "    words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos)) if get_wordnet_pos(pos) else word for word, pos in tagged]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "hatefulCorpus=[]\n",
    "nHatefulCorpus=[]\n",
    "neutralCorpus=[]\n",
    "with open(\"firstIter.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        if not all(value == \"\" for value in row.values()):\n",
    "            text = row[\"Comment Text\"]\n",
    "            newText = text.strip()\n",
    "            newText = replace_emojis(newText, emoji_dict)\n",
    "            newText = preprocess_text(newText)\n",
    "            label = row[\"Label\"]\n",
    "            if label=='Neutral':\n",
    "                neutralCorpus.append(newText)\n",
    "            elif label=='Hateful':\n",
    "                hatefulCorpus.append(newText)\n",
    "            else:\n",
    "                nHatefulCorpus.append(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5821ebaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 437 1976\n"
     ]
    }
   ],
   "source": [
    "print(str(len(neutralCorpus))+\" \"+str(len(hatefulCorpus))+\" \"+str(len(nHatefulCorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa1c54d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1547"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_texts = hatefulCorpus + nHatefulCorpus + neutralCorpus\n",
    "# Create frequency distribution\n",
    "word_freq = FreqDist()\n",
    "\n",
    "for text in all_texts:\n",
    "    words = text.split()\n",
    "    word_freq.update(words)\n",
    "known_words = {word for word, freq in word_freq.items() if freq > 3}\n",
    "len(known_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7414aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = 0\n",
    "num_comments = len(all_texts)\n",
    "\n",
    "for comment in all_texts:\n",
    "    total_words += len(comment.split())\n",
    "\n",
    "average_words = total_words / num_comments\n",
    "\n",
    "variance = 0\n",
    "for comment in all_texts:\n",
    "    num_words = len(comment.split())\n",
    "    variance += (num_words - average_words) ** 2\n",
    "\n",
    "std_deviation = math.sqrt(variance / num_comments)\n",
    "M = round(average_words + std_deviation)\n",
    "\n",
    "def comment_to_vector(comment, known_words, M):\n",
    "    words = comment.split()\n",
    "    vector = []\n",
    "    for i in range(M):\n",
    "        if i < len(words):\n",
    "            word = words[i]\n",
    "            if word in known_words:\n",
    "                vector.append(known_words.index(word) + 1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    return vector\n",
    "\n",
    "# Convert preprocessed comments to vector representation\n",
    "hateful_comment_vectors = []\n",
    "for comment in hatefulCorpus:\n",
    "    vector = comment_to_vector(comment, list(known_words), M)\n",
    "    hateful_comment_vectors.append(vector)\n",
    "    \n",
    "\n",
    "nHateful_comment_vectors = []\n",
    "for comment in nHatefulCorpus:\n",
    "    vector = comment_to_vector(comment, list(known_words), M)\n",
    "    nHateful_comment_vectors.append(vector)\n",
    "    \n",
    "neutral_comment_vectors = []\n",
    "for comment in neutralCorpus:\n",
    "    vector = comment_to_vector(comment, list(known_words), M)\n",
    "    neutral_comment_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34553cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hateful_comment_vectors[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39f4a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\"embedding dimension = %d should be divisible by number of heads = %d\" % (embed_dim, num_heads))\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "embed_dim = 32  # Embedding size for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90babcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 4s 24ms/step - loss: 0.7616 - accuracy: 0.7572 - val_loss: 0.6190 - val_accuracy: 0.8091\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.6569 - accuracy: 0.7841 - val_loss: 0.6053 - val_accuracy: 0.8091\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.6030 - accuracy: 0.8000 - val_loss: 0.7172 - val_accuracy: 0.7475\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.5464 - accuracy: 0.8114 - val_loss: 0.6259 - val_accuracy: 0.7972\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6259 - accuracy: 0.7972\n",
      "Test Loss: 0.6259426474571228\n",
      "Test Accuracy: 0.7972167134284973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, multi_head_self_attention_3_layer_call_fn, multi_head_self_attention_3_layer_call_and_return_conditional_losses, layer_normalization_6_layer_call_fn, layer_normalization_6_layer_call_and_return_conditional_losses while saving (showing 5 of 19). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: savedModels/Transformer_Model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: savedModels/Transformer_Model\\assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "class TransformerClassifier(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, maxlen, embed_dim, num_heads, ff_dim):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.pool = layers.GlobalAveragePooling1D()\n",
    "        self.classifier = layers.Dense(num_classes, activation='softmax', \n",
    "                               kernel_regularizer=regularizers.l2(0.01))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Parameters for the model\n",
    "vocab_size = 20000  \n",
    "maxlen = 200  \n",
    "embed_dim = 32  \n",
    "num_heads = 2  \n",
    "ff_dim = 32  \n",
    "\n",
    "hateful_comment_vectors = np.array(hateful_comment_vectors)\n",
    "nHateful_comment_vectors = np.array(nHateful_comment_vectors)\n",
    "neutral_comment_vectors = np.array(neutral_comment_vectors)\n",
    "\n",
    "all_comment_vectors = np.concatenate((hateful_comment_vectors, nHateful_comment_vectors, neutral_comment_vectors))\n",
    "\n",
    "hateful_labels = np.ones(len(hatefulCorpus))\n",
    "nHateful_labels = np.zeros(len(nHatefulCorpus))\n",
    "neutral_labels = np.full(len(neutralCorpus), 2)\n",
    "\n",
    "all_labels = np.concatenate((hateful_labels, nHateful_labels, neutral_labels))\n",
    "num_classes=3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_comment_vectors, all_labels, test_size=0.2, random_state=42)\n",
    "y_train_one_hot = to_categorical(y_train, num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerClassifier(vocab_size, maxlen, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train_one_hot, \n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test, y_test_one_hot), \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "model.save('savedModels/Transformer_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "441e7cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 669ms/step\n",
      "hateful\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "testData = \"All immigrants should be deported to where they came from.\"\n",
    "testData = testData.strip()\n",
    "testData = replace_emojis(testData, emoji_dict)\n",
    "testData = preprocess_text(testData)\n",
    "testDataVector = comment_to_vector(testData, list(known_words), 40)\n",
    "testDataVector = np.array(testDataVector)\n",
    "testDataVector = np.expand_dims(testDataVector, axis=0)\n",
    "loadedModel = tf.keras.models.load_model('savedModels/LSTM_Model')\n",
    "predicted_probabilities = loadedModel.predict(testDataVector)\n",
    "predicted_class = np.argmax(predicted_probabilities)\n",
    "class_labels = ['hateful', 'non-hateful', 'neutral']\n",
    "print(class_labels[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5142a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
